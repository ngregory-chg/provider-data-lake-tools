Two files with 10k rows and 2MB size:
- File parsing and cleaning: 5 seconds each file
- File preparation for training 16.5 min, 2.5 GB of RAM, 16-17% CPU
- Manual training 50 min: 52 records, 45 pos, 5 neg, 2 unsure
- Found 1,936 dupes in 30 seconds 

Two files 105K rows and 20MB:
- Use previous training file
- found 36,374 dupes in 5 min

Single File 14K rows and 1.4 MB
- File parsing 5 sec
- File preparation for training 150 sec, 400 MB of RAM
- Manual training 23 min: 162 records, 42 pos, 115 neg, 5 unsure
- Found 13,517 dupes in 2 seconds 

Single file 2.25 million rows and 235 MB
- File processing took 5min 30sec
- File deduping took 18 hours and did not finish

Single file 100k rows and 10.7 MB
- File processing took 10sec

Recommendations for future tasks
- Train ML to only focus on person specific data points (name, email, phone, address) and maybe NPI
- Separate dedupe into separate training and dedupe services
- Create service to combine clusters into single records aggregated by column
- Create uniform data set column definitions for other records (definitive, modio, locumsmart, etc)
- Create service to combine initial csv data into one csv before input to dedupe


Possible
- Run data set through dedupe to find records
- Create service to combine duplicate records into unique record data set/pool? 
-- Or create API around first pool with cluster dedupe IDS
---This API would faciliate the ability of users to 'clean' the pool but retrieving clusters and transforming them into a single record
-- API endpoint to add single records to the pool (check for potential match and upsert as applicable)
-- API endpoint to add bulk data sets to the pool